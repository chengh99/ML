{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Super parameters\n",
    "## ARGUMENTS\n",
    "epochs = 1\n",
    "dataset = 10\n",
    "how = \"normal\"\n",
    "action = \"eval\"\n",
    "threshold = 0.5\n",
    "contrast = 1\n",
    "weight = 7.0\n",
    "distort = False\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "homeDir = \"/Users/rockyemsn/Project/_ComputerVision/DDSM-Mammography\"\n",
    "model_name = \"model_s2.0.0.36b.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Batch generator with optional filenames parameter which will also return the filenames of the images\n",
    "## so that they can be identified\n",
    "def get_batches(X, y, batch_size, filenames=None, distort=False):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y))\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "    i, h, w, c = X.shape\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        batch_idx = shuffled_idx[i:i + batch_size]\n",
    "        X_return = X[batch_idx]\n",
    "\n",
    "        # do random flipping of images\n",
    "        coin = np.random.binomial(1, 0.5, size=None)\n",
    "        if coin and distort:\n",
    "            X_return = X_return[..., ::-1, :]\n",
    "\n",
    "        if filenames is None:\n",
    "            yield X_return, y[batch_idx]\n",
    "        else:\n",
    "            yield X_return, y[batch_idx], filenames[batch_idx]\n",
    "    return\n",
    " \n",
    "def _scale_input_data(X, contrast=None, mu=104.1353, scale=255.0):\n",
    "    # if we are adjusting contrast do that\n",
    "    if contrast and contrast != 1.0:\n",
    "        X_adj = tf.image.adjust_contrast(X, contrast)\n",
    "    else:\n",
    "        X_adj = X\n",
    "\n",
    "    # cast to float\n",
    "    if X_adj.dtype != tf.float32:\n",
    "        X_adj = tf.cast(X_adj, dtype=tf.float32)\n",
    "\n",
    "    # center the pixel data\n",
    "    X_adj = tf.subtract(X_adj, mu, name=\"centered_input\")\n",
    "\n",
    "    # scale the data\n",
    "    X_adj = tf.divide(X_adj, scale)\n",
    "\n",
    "    return X_adj\n",
    "\n",
    "\n",
    "\n",
    "## load weights from a checkpoint, excluding any or including specified vars and returning initializer function\n",
    "def load_weights(model_name, exclude=None, include=None):\n",
    "    model_path = os.path.join(\"model\", model_name + \".ckpt\")\n",
    "\n",
    "    variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=exclude, include=include)\n",
    "    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(model_path, variables_to_restore)\n",
    "\n",
    "    return init_fn\n",
    "\n",
    "## read data from tfrecords file\n",
    "def read_and_decode_single_example(filenames, label_type='label_normal', normalize=False, distort=False, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs)\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "\n",
    "    if label_type != 'label':\n",
    "        label_type = 'label_' + label_type\n",
    "\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    if label_type != 'label_mask':\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "                'label_normal': tf.FixedLenFeature([], tf.int64),\n",
    "                'image': tf.FixedLenFeature([], tf.string)\n",
    "            })\n",
    "\n",
    "        # extract the data\n",
    "        label = features[label_type]\n",
    "        image = tf.decode_raw(features['image'], tf.uint8)\n",
    "\n",
    "        # reshape and scale the image\n",
    "        image = tf.reshape(image, [299, 299, 1])\n",
    "\n",
    "        # random flipping of image\n",
    "        if distort:\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_flip_up_down(image)\n",
    "\n",
    "    else:\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                # We know the length of both fields. If not the\n",
    "                # tf.VarLenFeature could be used\n",
    "                'label': tf.FixedLenFeature([], tf.string),\n",
    "                'image': tf.FixedLenFeature([], tf.string)\n",
    "            })\n",
    "\n",
    "        label = tf.decode_raw(features['label'], tf.uint8)\n",
    "        image = tf.decode_raw(features['image'], tf.uint8)\n",
    "\n",
    "        label = tf.cast(label, tf.int32)\n",
    "        # image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\n",
    "        image = tf.reshape(image, [288, 288, 1])\n",
    "        label = tf.reshape(label, [288, 288, 1])\n",
    "\n",
    "        # if distort:\n",
    "        #     image, label = _image_random_flip(image, label)\n",
    "\n",
    "    if normalize:\n",
    "        image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    # return the image and the label\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional for me - No GPU \n",
    "# Function to do the data augmentation on the GPU instead of the CPU, doing it on the CPU significantly slowed down\n",
    "# training\n",
    "# Taken from https://becominghuman.ai/data-augmentation-on-gpu-in-tensorflow-13d14ecf2b19\n",
    "def augment(images, labels,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    "            augment_labels=False,\n",
    "            mixup=0):  # Mixup coeffecient, see https://arxiv.org/abs/1710.09412.pdf\n",
    "\n",
    "    # My experiments showed that casting on GPU improves training performance\n",
    "    if images.dtype != tf.float32:\n",
    "        images = tf.image.convert_image_dtype(images, dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope('augmentation'):\n",
    "        shp = tf.shape(images)\n",
    "        batch_size, height, width = shp[0], shp[1], shp[2]\n",
    "        width = tf.cast(width, tf.float32)\n",
    "        height = tf.cast(height, tf.float32)\n",
    "\n",
    "        # The list of affine transformations that our image will go under.\n",
    "        # Every element is Nx8 tensor, where N is a batch size.\n",
    "        transforms = []\n",
    "        identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n",
    "        if horizontal_flip:\n",
    "            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
    "            flip_transform = tf.convert_to_tensor(\n",
    "                [-1., 0., width, 0., 1., 0., 0., 0.], dtype=tf.float32)\n",
    "            transforms.append(\n",
    "                tf.where(coin,\n",
    "                         tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]),\n",
    "                         tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
    "\n",
    "        if vertical_flip:\n",
    "            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
    "            flip_transform = tf.convert_to_tensor(\n",
    "                [1, 0, 0, 0, -1, height, 0, 0], dtype=tf.float32)\n",
    "            transforms.append(\n",
    "                tf.where(coin,\n",
    "                         tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1]),\n",
    "                         tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])))\n",
    "\n",
    "        if transforms:\n",
    "            images = tf.contrib.image.transform(\n",
    "                images,\n",
    "                tf.contrib.image.compose_transforms(*transforms),\n",
    "                interpolation='BILINEAR')  # or 'NEAREST'\n",
    "\n",
    "            if augment_labels:\n",
    "                labels = tf.contrib.image.transform(\n",
    "                    labels,\n",
    "                    tf.contrib.image.compose_transforms(*transforms),\n",
    "                    interpolation='BILINEAR')  # or 'NEAREST'\n",
    "\n",
    "        def cshift(values):  # Circular shift in batch dimension\n",
    "            return tf.concat([values[-1:, ...], values[:-1, ...]], 0)\n",
    "\n",
    "        if mixup > 0:\n",
    "            beta = tf.distributions.Beta(mixup, mixup)\n",
    "            lam = beta.sample(batch_size)\n",
    "            ll = tf.expand_dims(tf.expand_dims(tf.expand_dims(lam, -1), -1), -1)\n",
    "            images = ll * images + (1 - ll) * cshift(images)\n",
    "            labels = lam * labels + (1 - lam) * cshift(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load the training data and return a list of the tfrecords file and the size of the dataset\n",
    "## Multiple data sets have been created for this project, which one to be used can be set with the type argument\n",
    "## /Users/rockyemsn/Project/_ComputerVision/DDSM-Mammography/data\n",
    "def get_training_data(what=10):\n",
    "    if what == 10:\n",
    "        train_path_10 = os.path.join(homeDir,\"data\", \"training10_0.tfrecords\")\n",
    "        train_path_11 = os.path.join(homeDir,\"data\", \"training10_1.tfrecords\")\n",
    "        train_path_12 = os.path.join(homeDir,\"data\", \"training10_2.tfrecords\")\n",
    "        train_path_13 = os.path.join(homeDir,\"data\", \"training10_3.tfrecords\")\n",
    "\n",
    "        train_files = [train_path_10, train_path_11, train_path_12, train_path_13]\n",
    "        total_records = 44712\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset!')\n",
    "\n",
    "    return train_files, total_records\n",
    "\n",
    "def get_test_data(what=10):\n",
    "    test_files = os.path.join(homeDir,\"data\", \"training10_4.tfrecords\")\n",
    "    \n",
    "    return [test_files], 11178\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _conv2d_batch_norm(input, filters, kernel_size=(3,3), stride=(1,1), training = tf.placeholder(dtype=tf.bool, name=\"is_training\"), epsilon=1e-8, padding=\"SAME\", seed=None, lambd=0.0, name=None, activation=\"relu\"):\n",
    "    with tf.name_scope('layer_'+name) as scope:\n",
    "        conv = tf.layers.conv2d(\n",
    "            input,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=stride,\n",
    "            padding=padding,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=seed),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lambd),\n",
    "            name='conv_'+name\n",
    "        )\n",
    "\n",
    "        # apply batch normalization\n",
    "        conv = tf.layers.batch_normalization(\n",
    "            conv,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn_'+name\n",
    "        )\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            # apply relu\n",
    "            conv = tf.nn.relu(conv, name='relu_'+name)\n",
    "        elif activation == \"elu\":\n",
    "            conv = tf.nn.elu(conv, name=\"elu_\" + name)\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Steps per epoch: 349\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "epsilon = 1e-8\n",
    "\n",
    "# learning rate\n",
    "epochs_per_decay = 5\n",
    "decay_factor = 0.80\n",
    "staircase = True\n",
    "\n",
    "# lambdas\n",
    "lamC = 0.00001\n",
    "lamF = 0.00250\n",
    "\n",
    "# use dropout\n",
    "dropout = True\n",
    "fcdropout_rate = 0.5\n",
    "convdropout_rate = 0.001\n",
    "pooldropout_rate = 0.1\n",
    "\n",
    "num_classes = 2\n",
    "train_files, total_records = get_training_data(what=dataset)\n",
    "test_files, total_records = get_test_data(what=dataset)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "steps_per_epoch = int(total_records / batch_size)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Build the graph\n",
    "graph = tf.Graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created...\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "    is_testing = tf.placeholder(dtype=bool, shape=(), name=\"is_testing\")\n",
    "\n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(0.001,\n",
    "                                               global_step,\n",
    "                                               1366,\n",
    "                                               decay_factor,\n",
    "                                               staircase=staircase)\n",
    "\n",
    "    with tf.name_scope('inputs') as scope:\n",
    "        image, label = read_and_decode_single_example(test_files, label_type=how, normalize=False, distort=False)\n",
    "\n",
    "        X_def, y_def = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=2000,\n",
    "                                              seed=None,\n",
    "                                              min_after_dequeue=1000)\n",
    "\n",
    "        # Placeholders\n",
    "        X = tf.placeholder_with_default(X_def, shape=[None, None, None, 1])\n",
    "        y = tf.placeholder_with_default(y_def, shape=[None])\n",
    "\n",
    "        # cast to float and scale input data\n",
    "        X_adj = tf.cast(X, dtype=tf.float32)\n",
    "        X_adj = _scale_input_data(X_adj, contrast=contrast, mu=127.0, scale=255.0)\n",
    "\n",
    "        # optional online data augmentation\n",
    "        if distort:\n",
    "            X_adj, y = augment(X_adj, y, horizontal_flip=True, vertical_flip=True, mixup=0)\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X_adj,\n",
    "            filters=32,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(2, 2),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=100),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv1'\n",
    "        )\n",
    "\n",
    "        conv1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(conv1, name='relu1')\n",
    "\n",
    "    with tf.name_scope('conv1.1') as scope:\n",
    "        conv11 = tf.layers.conv2d(\n",
    "            conv1_bn_relu,\n",
    "            filters=32,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=101),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv1.1'\n",
    "        )\n",
    "\n",
    "        conv11 = tf.layers.batch_normalization(\n",
    "            conv11,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1.1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv11 = tf.nn.relu(conv11, name='relu1.1')\n",
    "\n",
    "\n",
    "    with tf.name_scope('conv1.2') as scope:\n",
    "        conv12 = tf.layers.conv2d(\n",
    "            conv11,\n",
    "            filters=32,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1101),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv1.2'\n",
    "        )\n",
    "\n",
    "        conv12 = tf.layers.batch_normalization(\n",
    "            conv12,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1.2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv12 = tf.nn.relu(conv12, name='relu1.1')\n",
    "\n",
    "    # Max pooling layer 1\n",
    "    with tf.name_scope('pool1') as scope:\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv12,\n",
    "            pool_size=(3, 3), \n",
    "            strides=(2, 2),\n",
    "            padding='SAME',\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # optional dropout\n",
    "        if dropout:\n",
    "            pool1 = tf.layers.dropout(pool1, rate=pooldropout_rate, seed=103, training=training)\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    with tf.name_scope('conv2.1') as scope:\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,\n",
    "            filters=64,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=104),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv2.1'\n",
    "        )\n",
    "\n",
    "        conv2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2.1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv2 = tf.nn.relu(conv2, name='relu2.1')\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    with tf.name_scope('conv2.2') as scope:\n",
    "        conv22 = tf.layers.conv2d(\n",
    "            conv2,\n",
    "            filters=64,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1104),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv2.2'\n",
    "        )\n",
    "\n",
    "        conv22 = tf.layers.batch_normalization(\n",
    "            conv22,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2.2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv22 = tf.nn.relu(conv22, name='relu2.2')\n",
    "\n",
    "    # Max pooling layer 2\n",
    "    with tf.name_scope('pool2') as scope:\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv22,\n",
    "            pool_size=(2, 2),\n",
    "            strides=(2, 2),\n",
    "            padding='SAME',\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # optional dropout\n",
    "        if dropout:\n",
    "            pool2 = tf.layers.dropout(pool2, rate=pooldropout_rate, seed=106, training=training)\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    with tf.name_scope('conv3.1') as scope:\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            pool2,\n",
    "            filters=128,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=107),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv3.1'\n",
    "        )\n",
    "\n",
    "        conv3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3.1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv3 = tf.nn.relu(conv3, name='relu3.1')\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    with tf.name_scope('conv3.2') as scope:\n",
    "        conv32 = tf.layers.conv2d(\n",
    "            conv3,\n",
    "            filters=128,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1107),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv3.2'\n",
    "        )\n",
    "\n",
    "        conv32 = tf.layers.batch_normalization(\n",
    "            conv32,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3.2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv32 = tf.nn.relu(conv32, name='relu3.2')\n",
    "\n",
    "    # Max pooling layer 3\n",
    "    with tf.name_scope('pool3') as scope:\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            conv32,\n",
    "            pool_size=(2, 2),\n",
    "            strides=(2, 2),\n",
    "            padding='SAME',\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        if dropout:\n",
    "            pool3 = tf.layers.dropout(pool3, rate=pooldropout_rate, seed=109, training=training)\n",
    "\n",
    "    # Convolutional layer 4\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "            conv4 = tf.layers.conv2d(\n",
    "                pool3,\n",
    "                filters=256,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                padding='SAME',\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=110),\n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "                name='conv4'\n",
    "            )\n",
    "\n",
    "            conv4 = tf.layers.batch_normalization(\n",
    "                conv4,\n",
    "                axis=-1,\n",
    "                momentum=0.99,\n",
    "                epsilon=epsilon,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                beta_initializer=tf.zeros_initializer(),\n",
    "                gamma_initializer=tf.ones_initializer(),\n",
    "                moving_mean_initializer=tf.zeros_initializer(),\n",
    "                moving_variance_initializer=tf.ones_initializer(),\n",
    "                training=training,\n",
    "                name='bn4'\n",
    "            )\n",
    "\n",
    "            # apply relu\n",
    "            conv4_bn_relu = tf.nn.relu(conv4, name='relu4')\n",
    "\n",
    "    # Max pooling layer 4\n",
    "    with tf.name_scope('pool4') as scope:\n",
    "            pool4 = tf.layers.max_pooling2d(\n",
    "                conv4_bn_relu,\n",
    "                pool_size=(2, 2),\n",
    "                strides=(2, 2),\n",
    "                padding='SAME',\n",
    "                name='pool4'\n",
    "            )\n",
    "\n",
    "            if dropout:\n",
    "                pool4 = tf.layers.dropout(pool4, rate=pooldropout_rate, seed=112, training=training)\n",
    "\n",
    "    # Convolutional layer 5\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        conv5 = tf.layers.conv2d(\n",
    "            pool4,\n",
    "            filters=512,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=113),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv5'\n",
    "        )\n",
    "\n",
    "        conv5 = tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn5'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv5_bn_relu = tf.nn.relu(conv5, name='relu5')\n",
    "\n",
    "    # Max pooling layer 4\n",
    "    with tf.name_scope('pool5') as scope:\n",
    "        pool5 = tf.layers.max_pooling2d(\n",
    "            conv5_bn_relu,\n",
    "            pool_size=(2, 2),\n",
    "            strides=(2, 2),\n",
    "            padding='SAME',\n",
    "            name='pool5'\n",
    "        )\n",
    "\n",
    "        if dropout:\n",
    "            pool5 = tf.layers.dropout(pool5, rate=pooldropout_rate, seed=115, training=training)\n",
    "\n",
    "    fc1 = _conv2d_batch_norm(pool5, 2048, kernel_size=(5, 5), stride=(5, 5), training=training, epsilon=1e-8,\n",
    "                             padding=\"VALID\", seed=1013, lambd=lamC, name=\"fc_1\")\n",
    "\n",
    "    fc2 = _conv2d_batch_norm(fc1, 2048, kernel_size=(1, 1), stride=(1, 1), training=training, epsilon=1e-8,\n",
    "                             padding=\"VALID\", seed=1014, lambd=lamC, name=\"fc_2\")\n",
    "\n",
    "    fc3 = tf.layers.dense(\n",
    "        fc2,\n",
    "        num_classes,  # One output unit per category\n",
    "        activation=None,  # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=121),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"fc_logits\"\n",
    "    )\n",
    "\n",
    "    logits = tf.squeeze(fc3, name=\"fc_flat_logits\")\n",
    "\n",
    "    # get the fully connected variables so we can only train them when retraining the network\n",
    "    fc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"fc\")\n",
    "\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "        kernel_transposed = tf.transpose(conv_kernels1, [3, 0, 1, 2])\n",
    "\n",
    "    with tf.variable_scope('visualization'):\n",
    "        tf.summary.image('conv1/filters', kernel_transposed, max_outputs=32, collections=[\"kernels\"])\n",
    "\n",
    "    # This will weight the positive examples higher so as to improve recall\n",
    "    weights = tf.multiply(weight, tf.cast(tf.greater(y, 0), tf.float32)) + 1\n",
    "    mean_ce = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights))\n",
    "\n",
    "    # Add in l2 loss\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # get the probabilites for the classes\n",
    "    probabilities = tf.nn.softmax(logits, name=\"probabilities\")\n",
    "    abnormal_probability = 1 - probabilities[:,0]\n",
    "\n",
    "    # Compute predictions from the probabilities\n",
    "    if threshold == 0.5:\n",
    "        predictions = tf.argmax(probabilities, axis=1, output_type=tf.int32)\n",
    "    else:\n",
    "        predictions = tf.cast(tf.greater(abnormal_probability, threshold), tf.int32)\n",
    "\n",
    "    # get the accuracy\n",
    "    accuracy, acc_op = tf.metrics.accuracy(\n",
    "        labels=y,\n",
    "        predictions=predictions,\n",
    "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "        name=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    recall, rec_op = tf.metrics.recall(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"recall\")\n",
    "    precision, prec_op = tf.metrics.precision(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"precision\")\n",
    "\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('cross_entropy', mean_ce, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('learning_rate', learning_rate, collections=[\"summaries\"])\n",
    "\n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all(\"summaries\")\n",
    "\n",
    "    print(\"Graph created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check tensorflow version to see if we need another virtual kernel \n",
    "print(\"Tensorflow version: \" + tf.__version__)\n",
    "\n",
    "## CONFIGURE OPTIONS\n",
    "init = False\n",
    "print_every = 5  # how often to print metrics\n",
    "checkpoint_every = 1  # how often to save model in epochs\n",
    "print_metrics = True  # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
    "\n",
    "config = tf.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./model: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# copy the checkpoints\n",
    "!mkdir ./model\n",
    "\n",
    "# Scan training set PATH to be defined\n",
    "#!cp ../data/fcn-trained-on-ddsm-images/* ./model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/rockyemsn/Project/_ComputerVision/DDSM-Mammography/model/model_s2.0.0.36b.10.ckpt\n",
      "Restoring model model_s2.0.0.36b.10\n",
      "Test Accuracy: 0.97925645\n",
      "Test Recall: 0.9501405\n",
      "Test Precision: 0.89424986\n"
     ]
    }
   ],
   "source": [
    "## action = \"init\"\n",
    "## action = \"train\"\n",
    "action = \"init\"\n",
    "\n",
    "## init = True\n",
    "## Initializing model...\n",
    "## Test Accuracy: 0.13703305\n",
    "## Test Recall: 0.96151155\n",
    "## Test Precision: 0.12572056\n",
    "init = False\n",
    "\n",
    "## train the model\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # If the model is new initialize variables, else restore the session\n",
    "    if init:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Initializing model...\")\n",
    "    else:\n",
    "        # saver = tf.train.import_meta_graph('/tmp/model.ckpt.meta')\n",
    "        # saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        # saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "        # saver = tf.train.import_meta_graph('/Users/rockyemsn/Project/DDSM-Mammography/model/model_s2.0.0.36b.10.ckpt.meta')\n",
    "        # new_saver.restore(s, tf.train.latest_checkpoint('../MY_MODELS'))\n",
    "        # saver.restore(sess, tf.train.latest_checkpoint(homeDir + '/model/' + model_name + '.ckpt'))\n",
    "        saver.restore(sess, homeDir + '/model/' + model_name + '.ckpt')\n",
    "        \n",
    "        print(\"Restoring model\", model_name)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "        \n",
    "    # if we are training the model\n",
    "    if action == \"train\":\n",
    "\n",
    "        print(\"Training model\", homeDir + '/model/' + model_name, \"...\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            for i in range(steps_per_epoch):\n",
    "                # create the metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "\n",
    "                _, precision_value, summary, acc_value, cost_value, recall_value = sess.run(\n",
    "                    [extra_update_ops, prec_op, merged, accuracy, mean_ce, rec_op],\n",
    "                    feed_dict={\n",
    "                        training: True,\n",
    "                    },\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata)\n",
    "                \n",
    "            # save checkpoint every nth epoch\n",
    "            if (epoch % checkpoint_every == 0):\n",
    "                print(\"Saving checkpoint\")\n",
    "                save_path = saver.save(sess, homeDir + '/model/' + model_name + '.ckpt')\n",
    "\n",
    "                # Now that model is saved set init to false so we reload it next time\n",
    "                init = False\n",
    "    else:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        \n",
    "        # evaluate the test data\n",
    "        for i in range(steps_per_epoch-1):\n",
    "            valid_acc, valid_recall, valid_precision = sess.run(\n",
    "                [acc_op, rec_op, prec_op],\n",
    "                feed_dict={\n",
    "                    training: False\n",
    "                })\n",
    "\n",
    "        # evaluate once more to get the summary\n",
    "        cv_recall, cv_precision, cv_accuracy = sess.run(\n",
    "            [recall, precision, accuracy],\n",
    "            feed_dict={\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(\"Test Accuracy:\", cv_accuracy)\n",
    "        print(\"Test Recall:\", cv_recall)\n",
    "        print(\"Test Precision:\", cv_precision)\n",
    "        \n",
    "    # stop the coordinator and the threads\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
